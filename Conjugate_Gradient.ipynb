{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjugate Gradient Descent\n",
    "\n",
    "Conjugate gradient methods are suitable for optimization problems of the type: $f(x)\\ =\\ x^TAx\\ -\\ x^Tb$, where A is a real, symmetric and positive definite matrix, and x and b are row vectors. x and b are 1\\*n and A is n\\*n.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative of f(x) is a well-known result from Vector Calculus:\n",
    "\n",
    "$$f'(x)\\ =\\ x^T(A\\ +\\ A^T)/2\\ -\\ b\\ =\\ Ax\\ -\\ b\\ (since\\ A\\ is\\ symmetric,\\ A=A^T).\\ Let\\ x^*\\ =\\ argmin_x f(x).\\ Then\\ we\\ set\\ f'(x^*) = 0,\\ =>\\ Ax^*\\ =\\ b.$$\n",
    "\n",
    "In other words, the value of x which mimimizes f(x) is given by the solution to the linear system of equations Ax = b.\n",
    "\n",
    "This is one of the most important problems in Linear Algebra, and so there are a number of ways to approach it. \n",
    "\n",
    "## Iterative Solution to Ax = b\n",
    "\n",
    "Conjugate Gradient method is an iterative method to solve the Ax = b problem that can be useful when A is large and sparse. The idea is that in each iteration, we go in a direction that is conjugate w.r.t A (i.e. A-orthogonal) to the current gradient. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's define conjugacy: Two vectors x & y are conjugate with respect to A if $x^TAy\\ =\\ 0$.\n",
    "\n",
    "Imagine that $(p_1,\\ p_2,\\ ..., p_n)$ are a set of vectors that are mutually conjugate w.r.t A. Then, since the row/column space of A is $\\mathbb{R}^n$, they are also a basis of $\\mathbb{R}^n$. So, we can represent any real vector of length n (e.g. x) as a linear combination of the $\\alpha$s. If x* is the solution of Ax = b, then:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& x^* = \\sum_{k=1}^n \\alpha_kp_k \\\\\n",
    "&=> Ax^* = \\sum_{k=1}^n\\ \\alpha_kAp_k,\\ multiplying\\ both\\ sides\\ by\\ A \\\\\n",
    "&=> p_i^{T}Ax^* = \\sum_{k\\ =\\ 1}^n\\ \\alpha_kp_i^{T}Ap_k,\\ multiplying\\ both\\ sides\\ by\\ some\\ p_{i}^T \\\\\n",
    "&=> p_i^{T}Ax^* = \\alpha_ip_i^{T}Ap_i,\\ because\\ p_i\\ and\\ p_k\\ are\\ elements\\ of\\ an\\ orthogonal\\ basis,\\ p_i^{T}Ap_k = 0\\ if\\ i\\ \\ne k \\\\\n",
    "&=> \\alpha_i = \\frac{p_i^{T}Ax^*} {p_i^{T}Ap_i} = \\frac{p_i^{T}b} {p_i^{T}Ap_i},\\ since\\ Ax*= b\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This gives us a way to iteratively build the solution:\n",
    "\n",
    "1. Incrementally find $p_i$ and $\\alpha_i$\n",
    "2. $x_{i+1} = x_i + \\alpha_ip_i$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Direction and Residuals\n",
    "\n",
    "The $p_i$s are called 'search directions'.\n",
    "\n",
    "But how to find the conjugate gradient in each step? For this, we employ a technique in Linear Algebra called Gram-Schmidt Orthogonalization.\n",
    "\n",
    "Let's define the residual at the k-th iteration, $r_k$ as the negative of the gradient $Ax_k - b$. We want to establish a relation between $r_k$ and $r_{k+1}$. \n",
    "\n",
    "$$\\begin{align}\n",
    "& r_k = b - Ax_k \\\\\n",
    "& r_{k+1} = b - Ax_{k+1} \\\\\n",
    "&= r_k + Ax_k - Ax_{k+1} \\\\\n",
    "&= r_k - A(x_{k+1} - x_k) \\\\\n",
    "&= r_k - \\alpha_kAp_k\n",
    "\\end{align}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also useful to express $\\alpha$ in terms of the $r_k$s: $\\alpha_k = -(r_{k+1} - r_k) / Ap_k$\n",
    "\n",
    "## Properties of the Residual\n",
    "\n",
    "The residuals have two important properties: \n",
    "\n",
    "1. The residual at the k-th iteration $r_k$ is independent (i.e. orthogonal) of the previous search directions, $(p_0, ...p_{k-1})$. To see why:\n",
    "\n",
    "$$\\begin{align}\n",
    "&r_k = b - Ax_k \\\\\n",
    "& = Ax_n - Ax_k \\\\ \n",
    "&= A(x_n - x_k) \\\\ \n",
    "&= A(\\sum_{i=0}^{n-1} \\alpha_ip_i - \\sum_{i=0}^{k-1} \\alpha_ip_i) \\\\\n",
    "&= \\sum_{i=k}^{n-1} \\alpha_iAp_i \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "If we multiply both sides by $p_j^T$, where $j < k$, we get:\n",
    "\n",
    "$$p^T_jr_k = \\sum_{i=k}^{n-1} \\alpha_ip^T_jAp_i = 0,\\ since\\ p^T_jAp_i = 0\\ if\\ i \\ne j$$\n",
    "\n",
    "2. The residual at the k-th iteration $r_k$ is independent (i.e. orthogonal) of the previous residuals $(r_0, ...r_{k-1})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build an Orthogonal Basis - Gram Schmidt Rule\n",
    "\n",
    "Now we're ready to apply the Gram-Schmidt rule, which says given a set of vectors $(u_1,...,u_n)$, a corresponding set of vectors $(v_1,...,v_n)$ that are orthogonal in the inner product space can be found by:\n",
    "\n",
    "$$\\begin{align}\n",
    "& v_1 = u_1 \\\\\n",
    "& v_{n} = u_{n} - \\sum_{i = 1}^{n-1} \\frac {<u_n, v_i>}{<v_i, v_i>} v_i\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Suppose we're at the k-th iteration. Applied to our case, the input to the Gram-Schmidt rule is the set of residuals $(r_0, r_1, ..., r_k)$. The outputs are:\n",
    "\n",
    "$$\\begin{align}\n",
    "& p_0 = r_0 \\\\\n",
    "& p_k = r_{k} - \\sum_{i = 0}^{k-1} \\frac {r_{k}^TAp_i}{p_i^TAp_i} p_i \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "But we had proved earlier that the residual is independent of the previous search direction. So, the numerators of the form $r_{k}^TAp_i$ are 0 except when $i = k-1$. So the summation reduces to just one term.\n",
    "\n",
    "$$p_k = r_{k} - \\frac {r_{k}^TAp_{k-1}} {p_{k-1}^TAp_{k-1}} p_{k-1}$$\n",
    "\n",
    "Let's call the coefficient of $p_{k-1}$, $\\beta_{k-1}$. So we have $p_k = r_{k} + \\beta_{k-1}p_{k-1}$ with $\\beta_{k-1} = - \\frac {r_{k}^TAp_{k-1}} {p_{k-1}^TAp_{k-1}}$.\n",
    "\n",
    "## Simplifying $\\alpha$ and $\\beta$\n",
    "\n",
    "We can further simplify the numerator of $\\beta_{k-1}$ by expanding $Ap_{k-1}$:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "r_{k}^TAp_{k-1} \n",
    "&= r_{k}^T [\\frac{r_{k-1} - r_{k}} {\\alpha_{k-1}}],\\ using\\ r_{k} = r_{k-1} - \\alpha_{k-1}Ap_{k-1} \\\\\n",
    "&= - \\frac{r_{k}^Tr_{k}} {\\alpha_{k-1}},\\ since\\ r_k\\ and\\ r_{k-1}\\ are\\ orthogonal. \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Simialrly, we can simplify the denominator:\n",
    "$$\\begin{align}\n",
    "p_{k-1}^TAp_{k-1}\n",
    "&= [r_{k-1} + \\beta_{k-2}p_{k-2}]^T Ap_{k-1} \\\\\n",
    "&= r_{k-1}^TAp_{k-1} \\\\\n",
    "&= r_{k-1}^T[\\frac{r_{k-1} - r_{k}} {\\alpha_{k-1}}] \\\\\n",
    "&= \\frac{r_{k-1}^Tr_{k-1}} {\\alpha_{k-1}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We use the last result to obtain the simplified expressions for both $\\alpha$ and $\\beta$:\n",
    "\n",
    "$$\\begin{align}\n",
    "& \\beta_{k-1} = - \\frac{r_{k}^Tr_{k}} {r_{k-1}^Tr_{k-1}} \\\\\n",
    "& \\alpha_{k-1} = \\frac{r_{k-1}^Tr_{k-1}} {p_{k-1}^TAp_{k-1}} \\\\\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A is both symmetric & positive definite: True\n",
      "Solution with inverse: [[ 4.75  7.5   6.25]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import cholesky, inv, LinAlgError\n",
    "\n",
    "def is_symmetric(x):\n",
    "    return np.all((x-x.T) == 0)\n",
    "\n",
    "def is_positive_definite(x):\n",
    "    pos_def = False\n",
    "    try:\n",
    "        cholesky(x)\n",
    "        pos_def = True\n",
    "    except LinAlgError:\n",
    "        return False\n",
    "    return pos_def\n",
    "\n",
    "A = np.matrix([[2, -1, 0], \n",
    "               [-1, 2, -1], \n",
    "               [0, -1, 2]])\n",
    "\n",
    "print('A is both symmetric & positive definite: %s' % (is_symmetric(A) and is_positive_definite(A),))\n",
    "\n",
    "b = np.atleast_2d([2, 4, 5]).T\n",
    "\n",
    "# Use the inverse method to find the expected solution:\n",
    "expected = np.squeeze(inv(A)*b, axis=1)\n",
    "\n",
    "print('Solution with inverse: %s' % (expected,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Solution using conjugate gradient: [[ 4.75  7.5   6.25]]\n",
      "Minimum value of f(x): [[-35.375]]\n"
     ]
    }
   ],
   "source": [
    "def f(A, b, x):\n",
    "    return (0.5*x.T*A*x) - (x.T*b)\n",
    "\n",
    "def conjugate_gradient(A, b):\n",
    "    x_init = np.atleast_2d(np.zeros(A.shape[0])).T\n",
    "    x_new, x = x_init, x_init\n",
    "    \n",
    "    r = b - A*x\n",
    "    r_new = r\n",
    "    \n",
    "    p = r.copy()   \n",
    "    n_iters = A.shape[0]\n",
    "    \n",
    "    for i in range(n_iters):\n",
    "        alpha = np.asscalar((r.T*r) / (p.T*A*p))\n",
    "        x_new = x + alpha*p\n",
    "        r_new = r - alpha*A*p\n",
    "        \n",
    "        beta = np.asscalar((r_new.T*r_new) / (r.T*r))        \n",
    "        p = r_new + beta*p        \n",
    "        x, r = x_new, r_new\n",
    "    \n",
    "    return x\n",
    "\n",
    "calc_x_min = conjugate_gradient(A, b)\n",
    "print('\\nSolution using conjugate gradient: %s' % np.squeeze(calc_x_min, axis=1))\n",
    "print('Minimum value of f(x): %s' % (f(A, b, calc_x_min),))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
