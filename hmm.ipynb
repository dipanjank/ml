{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h1 align=\"center\">Hidden Markov Model and Application to ASR</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "HMMs are probabilistic sequence  classifiers. A sequence classifier is a model whose job is to assign some label or class to each unit in a sequence. Given a sequence of units (words, letters, morphemes, sentences, whatever) their job is to compute a probability distribution over possible labels and choose the best label sequence.\n",
    "\n",
    "<img src=\"http://www.cs.virginia.edu/%7Ehw5x/Course/CS6501-Text-Mining/_site/docs/codes/HMM.PNG\" width=\"400\" height=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* The $t_i$s are instances of the hidden or latent variable. \n",
    "* The $w_i$s are instances of the tangible 'output' variable.\n",
    "\n",
    "For example, in a part of speech tagger, the $w_i$s are the words and the $t_i$s are the parts of speech we wish to assign to these words.\n",
    "\n",
    "**Markov Assumption:** Current state $t_i$ only depends on previous k tags for a k-th order HMM. Assuming an HMM of order 1,\n",
    "\n",
    "$$\\begin{equation}\n",
    "P(t_i \\mid t_1, t_2, \\ldots t_{i-1}) = P(t_i  \\mid t_{i-1})\n",
    "\\end{equation}$$\n",
    "\n",
    "**Output Independence Assumption:** Current output state $w_i$ only depends on the current hidden state $t_i$. Assuming an HMM of order 1,\n",
    "\n",
    "$$\\begin{equation}\n",
    "P(w_1, w_2,\\ldots w_i, t_1, t_2, \\ldots t_i) = P(w_i \\mid t_i)\n",
    "\\end{equation}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Components of a Hidden Markov Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Next, we'll describe the different components of an HMM using the Ice Cream task. \n",
    "\n",
    "    Imagine that you are a climatologist in the year 2799 studying the history of global warming. You cannot find any records of the weather for the summer of 2007, but you do find Jason Eisner’s diary, which lists how many ice creams Jason ate every day that summer. Our goal is to use these observations to estimate the temperature every day. \n",
    "\n",
    "We’ll simplify this weather task by assuming there are only two kinds of days: cold (C) and hot (H). Also, Jason cares about his health so he only eats 1-3 icecreams. So the Eisner task is as follows:\n",
    "\n",
    "Given a sequence of observations, each observation an integer between 1-3, corresponding to the number of ice creams eaten on a given day, figure out the correct ‘hidden’ sequence of weather states (H or C) which caused Jason to eat the ice cream. \n",
    "\n",
    "<img src=\"https://qph.ec.quoracdn.net/main-qimg-8fce62d562ac08766c168507f194956c\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## The 3 HMM Problems\n",
    "\n",
    "Hidden Markov Models are characterized by three fundamental problems:\n",
    "\n",
    "**Problem 1 (Computing Likelihood):** Given an HMM $\\lambda = (A, B)$ and an observation sequence O, determine the likelihood $P(O \\mid \\lambda)$.\n",
    "\n",
    "**Problem 2 (Decoding):** Given an HMM $\\lambda = (A, B)$ and an observation sequence O, find the most likely sequence Q.\n",
    "\n",
    "**Problem 3 (Learning):** Given an HMM $\\lambda = (A, B)$ and an observation sequence O, learn the HMM parameters A and B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Computing Likelihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's calculate the probability of the observation equence {3, 1} from the ice-cream HMM. This is the marginal probability\n",
    "\n",
    "$P(O) = \\sum_Q P(O, Q) = P(O \\mid Q) \\times P(Q) = \\sum_Q [\\prod_{i=1}^T P(o_i \\mid q_i) \\times \\prod_{i=1}^T P(q_i | q_{i-1})] $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "$P(3,1) = P(3, 1, H, H) + P(3, 1, H, C) + P(3, 1, C, H) + P(3, 1, C, C)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "$P(3, 1, H, H) = P(3 \\mid H) \\times P(1 \\mid H) \\times P(H \\mid H) \\times P(H) = 0.4 \\times 0.2 \\times 0.7 \\times 0.5 = 0.028$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(3, 1, H, C) = P(3 \\mid H) \\times P(1 \\mid C) \\times P(H \\mid C) \\times P(H) = 0.4 \\times 0.5 \\times 0.4 \\times 0.5 = 0.04$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(3, 1, C, H) = P(3 \\mid C) \\times P(1 \\mid H) \\times P(C \\mid H) \\times P(C) = 0.1 \\times 0.2 \\times 0.3 \\times 0.5 = 0.003$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(3, 1, C, C) = P(3 \\mid C) \\times P(1 \\mid C) \\times P(C \\mid C) \\times P(C) = 0.1 \\times 0.5 \\times 0.6 \\times 0.5 = 0.015$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, $P(3, 1) = 0.028 + 0.04 + 0.003 + 0.015 = 0.086$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "While computing $P(O \\mid \\lambda)$ is this way is simple, for an HMM with T steps and N values for the hidden state, this computation take $O(T \\times N^T)$ time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The Dynamic Programming solution to this problem is called the Forward Algorithm.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"https://danieltakeshi.github.io/assets/forward_trellis.png\" /></td>\n",
    "        <td><img src=\"hmm_forward.PNG\" /></td>\n",
    "    </tr>\n",
    "</table>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the complexity of the forward algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Decoding: The Viterbi Algorithm\n",
    "\n",
    "Since we already know how to calculate the likelihood of a given observation sequence $P(O \\mid Q)$, the most likely subsequence is $argmax_{Q} P(O \\mid Q)$\n",
    "\n",
    "However, since the Forward algorithm takes $O(T \\times N^2)$ time to calculate the likelihood of a given observation sequence and there are $T^N$ subsequences to evaluate, the direct evaluation method again has exponential time complexity. \n",
    "\n",
    "The solution is to look for the most likely sequence directly using Dynamic Programming.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"viterbi_trellis.png\" /></td>\n",
    "        <td><img src=\"viterbi_defn.PNG\" /></td>\n",
    "    </tr>\n",
    "</table>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning: The Baum-Welch Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "The example and diagrams are from <a href=\"http://www.mit.edu/~6.863/spring2011/jmnew/6.pdf\">Speech and Language Processing:  An introduction to natural language processing, computational linguistics, and speech recognition.  Daniel Jurafsky & James H. Martin.</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
