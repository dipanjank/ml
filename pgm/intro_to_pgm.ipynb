{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align='center'>Introduction to Probabilistic Graphical Models </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probabilistic graphical models are an elegant framework which combines uncertainty (probabilities) and logical structure (independence constraints) to compactly represent complex, real-world phenomena."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joint Probability Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main tool for probabilistic inference for any problem is the joint probability distribution.\n",
    "\n",
    "## The Student Example\n",
    "\n",
    "<img src=\"student_1.png\" width=\"400\" height=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "pylab.style.use('ggplot')\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us assume that the random variables take the following values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "difficulty = ['Easy', 'Hard']\n",
    "intelligence = ['Avg', 'Below Avg', 'Above Avg']\n",
    "grade = ['A', 'B', 'C']\n",
    "sat = ['Good', 'Bad']\n",
    "letter = ['Strong', 'Lukewarm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "rows = [p for p in product(difficulty, intelligence, grade, sat, letter)]\n",
    "rows_df = pd.DataFrame(rows, columns=['difficulty', 'intelligence', 'grade', 'sat', 'letter'])\n",
    "probs = np.random.uniform(0, 1, len(rows_df))                          \n",
    "rows_df = rows_df.assign(p=probs/probs.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>difficulty</th>\n",
       "      <th>intelligence</th>\n",
       "      <th>grade</th>\n",
       "      <th>sat</th>\n",
       "      <th>letter</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Easy</td>\n",
       "      <td>Avg</td>\n",
       "      <td>A</td>\n",
       "      <td>Good</td>\n",
       "      <td>Strong</td>\n",
       "      <td>0.023762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Easy</td>\n",
       "      <td>Avg</td>\n",
       "      <td>A</td>\n",
       "      <td>Good</td>\n",
       "      <td>Lukewarm</td>\n",
       "      <td>0.025619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Easy</td>\n",
       "      <td>Avg</td>\n",
       "      <td>A</td>\n",
       "      <td>Bad</td>\n",
       "      <td>Strong</td>\n",
       "      <td>0.006901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Easy</td>\n",
       "      <td>Avg</td>\n",
       "      <td>A</td>\n",
       "      <td>Bad</td>\n",
       "      <td>Lukewarm</td>\n",
       "      <td>0.005887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Easy</td>\n",
       "      <td>Avg</td>\n",
       "      <td>B</td>\n",
       "      <td>Good</td>\n",
       "      <td>Strong</td>\n",
       "      <td>0.005623</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  difficulty intelligence grade   sat    letter         p\n",
       "0       Easy          Avg     A  Good    Strong  0.023762\n",
       "1       Easy          Avg     A  Good  Lukewarm  0.025619\n",
       "2       Easy          Avg     A   Bad    Strong  0.006901\n",
       "3       Easy          Avg     A   Bad  Lukewarm  0.005887\n",
       "4       Easy          Avg     B  Good    Strong  0.005623"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72, 6)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For most practical applications, the joint probability distribution is over a ombinatorial space. This prohibits both learning and inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Independencies Reduce the Joint PD\n",
    "\n",
    "If all the variables were independent, we could simply the joint probability distribution to:\n",
    "\n",
    "$P(D, I, G, S, L) = P(D) P(I) P(G) P(S) P(L)$\n",
    "\n",
    "which requires only 12 entries instead of 72."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general however, we're not so lucky. Encoding the dependency structure in a graph allows us to take advantage of the efficient computation techniques from graph theory and apply them in solving the learning and inference problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two Kinds of PGMs\n",
    "\n",
    "If the dependencies are causal, then the representation is a directed graph (usually DAG). This is called a **Belief Network** or a **Bayesian Network**.\n",
    "\n",
    "If the dependencies signify relatedness but not causality, then the representation is an undirected graph. This is called a **Markov Network**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observed and Latent Variables\n",
    "\n",
    "A PGM can have two types of nodes - Observed and Latent.\n",
    "\n",
    "An Observed node represents a random variable whose output can be measured e.g. Grade.\n",
    "\n",
    "A latent node represents a random variable whose values cannot be seen or measured. Instead we learn the variable (i.e. it's probability distribution) from data. For a known structure, this is the learning problem.\n",
    "\n",
    "For example, an HMM (a type of Bayesian Network) for a PoS tagger represents the words in a sentence by observed nodes and the corresponding PoS as a latent node.\n",
    "\n",
    "<img src=\"hmm_pos_tagger.PNG\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Independencies in Bayesian Network\n",
    "\n",
    "## Condition Independence\n",
    "\n",
    "True indpendence between variables are hard to find. In reality, conditional independence is more useful. \n",
    "\n",
    "Given three variables $X, Y, Z: \\space X \\perp Y \\mid Z \\text{ iff } P(X, Y | Z) = P(X \\mid Z) P(Y \\mid Z)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active Trails\n",
    "\n",
    "Active trails determine which nodes in a Bayesian Network are able to influence each other.\n",
    "\n",
    "Let X, Y and Z be three nodes in a Bayesian Network. Then we have the following relationship cases:\n",
    "\n",
    "* Indirect Causal Effect: $X \\rightarrow Y \\rightarrow Z$ => $X \\perp Y \\mid Z$\n",
    "\n",
    "* Indirect Evidential Effect: $X \\leftarrow Y \\leftarrow Z$ => $X \\perp Y \\mid Z$\n",
    "\n",
    "* Common Cause: $X \\leftarrow Y \\rightarrow Z$ => $X \\perp Y \\mid Z$\n",
    "\n",
    "* Common Effect: $X \\rightarrow Y \\leftarrow Z$ => $X \\perp Y$ if $Z$ is not observed.\n",
    "\n",
    "Active trail is the generalization of the above idea. Formally, an active trail is a path through a Bayesian network such that for any 3 consecutive nodes $X_{i-1}, X_i, X_{i+1}$, none of the above independence relationships hold. In other words, an active trail is a flow of causal inference from end to end.\n",
    "\n",
    "## d-Separation\n",
    "\n",
    "Let X, Y and Z be three sets of nodes in a Bayesian Network. X and Y are d-separated if there are no active trails between them when $Z$ is observed.\n",
    "\n",
    "d-Separation converts the problem of statistical indpendence to graph independence. If P factorizes over G and $d-sep_G(X, Y \\mid Z)$ then P satisfies $X \\perp Y \\mid Z$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem:** In a Bayesian Network $(P, G)$, each node is independent of non-descendants given its immediate parents.\n",
    "\n",
    "Which leads to:\n",
    "\n",
    "**Chain Rule for Bayesian Network:** In a Bayesian network $(P, G)$, \n",
    "\n",
    "$$P(x_1, x_2, \\cdots x_k) = \\prod_{i=1}^k P(x_i \\mid Pa_{G}(x_i))$$\n",
    "\n",
    "In this case, we say P factorizes over G."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying this to the student graph, \n",
    "\n",
    "$P(D, I, G, S, L) = P(D)P(I)P(G \\mid D, I)P(S \\mid I)P(L \\mid G)$\n",
    "\n",
    "<img src=\"student_cpd.PNG\" width=\"500\" height=\"600\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every probability distribution always has at least one graph representation (which one?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I-Map\n",
    "\n",
    "The set of all independencies in P are: $I(P) = \\{(X \\perp Y \\mid Z) : P \\models (X \\perp Y \\mid Z)\\}$.\n",
    "\n",
    "**Theorem:** If P factorizes over G, then G is an I-map of P.\n",
    "\n",
    "The converse is also true.\n",
    "\n",
    "**Theorem (Hammersley-Clifford):** If G is an I-map of P, then P factorizes over G.\n",
    "\n",
    "Note that $I(G) \\subseteq I(P)$, i.e. P can contain indpendencies that are not in G.\n",
    "\n",
    "## Minimal I-Map\n",
    "\n",
    "Usually we are interested in the sparsest representation possible.\n",
    "\n",
    "G is a minimal I-map of P if removing any edge from G introduces independencies that are not in P.\n",
    "\n",
    "## Perfect I-Map\n",
    "\n",
    "G is a perfect I-map of P if I(G) = I(P). Not all distributions have a perfect I-map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indepdendencies in Markov Network\n",
    "\n",
    "Markov Networks don't have a notion of d-separation, there's only separation.\n",
    "\n",
    "Let X, Y and Z be mututally exclusive sets of nodes in a Markov Network (P, H). Then X and Y are separated given Z if there are no active trails between X and Y in H when Z is observed.\n",
    "\n",
    "For a Markov Network, an active trail is a path through the Network along which no nodes are observed.\n",
    "\n",
    "**Representation Theorem for Markov Networks:** If P factorizes over H and $sep_H(X, Y \\mid Z)$, then P satisfies $(X \\perp Y \\mid Z)$. \n",
    "\n",
    "**Theorem:** If P factorizes over H, H is an I-map for P.\n",
    "\n",
    "Example: Which independencies are in I(H) for the Markov Network below?\n",
    "\n",
    "<img src=\"mn_example.PNG\" />\n",
    "\n",
    "* $(A \\perp F \\mid B, D)$\n",
    "* $(A \\perp E \\mid B)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameterization in Markov Networks\n",
    "\n",
    "Parameterization in MNs is different from BNs because MNs do not have the notion of conditioning - inference flows in both directions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factors\n",
    "\n",
    "A factor of a set of random variables $(x_1, \\cdots, x_k)$ is a function that maps a particular assignment of $(x_1, \\cdots, x_k)$ to $\\mathbb{R}.$\n",
    "\n",
    "$\\phi: Val(x_1, \\cdots, x_k) \\rightarrow \\mathbb{R}$\n",
    "\n",
    "The set $(x_1, \\cdots, x_k)$ is called the scope of the factor.\n",
    "\n",
    "The CPDs in BNs are a special case of the factor, but in general factors do not make a probability distribution.\n",
    "\n",
    "Example: Let X and Y be two binary valued random variables."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
