{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flavours of Gradient Descent\n",
    "\n",
    "A quick recap of the Gradient Descent method: This is an iterative algorithm to minize a loss function $L(x)$, where we start with a guess of what the answer should be - and then take steps proportional to the gradient at the current point.\n",
    "\n",
    "$x = x_0$ (initial guess)\n",
    "\n",
    "Until Convergence is achieved:\n",
    "    \n",
    "$x_{i+1} = x_{i} - \\eta\\nabla_L(x_i)$\n",
    "\n",
    "For example, Let's say $L(x) = x^2 - 2x + 1$ and we start at $x0 = 2$. Coding the Gradient Descent method in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum value of L(x) = x**2 - 2*x + 1.0 is [0.00] at x = [1.00]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def L(x):\n",
    "    return x**2 - 2*x + 1\n",
    "\n",
    "def L_prime(x):\n",
    "    return 2*x - 2\n",
    "\n",
    "\n",
    "def converged(x_prev, x, epsilon):\n",
    "    \"Return True if the abs value of all elements in x-x_prev are <= epsilon.\"\n",
    "    \n",
    "    absdiff = np.abs(x-x_prev)\n",
    "    return np.all(absdiff <= epsilon)\n",
    "\n",
    "\n",
    "def gradient_descent(f_prime, x_0, learning_rate=0.2, n_iters=100, epsilon=1E-8):\n",
    "    x = x_0\n",
    "    \n",
    "    for _ in range(n_iters):\n",
    "        x_prev = x\n",
    "        x -= learning_rate*f_prime(x)\n",
    "        \n",
    "        if converged(x_prev, x, epsilon):\n",
    "            break\n",
    "            \n",
    "    return x\n",
    "\n",
    "x_min = gradient_descent(L_prime, 2)\n",
    "\n",
    "print('Minimum value of L(x) = x**2 - 2*x + 1.0 is [%.2f] at x = [%.2f]' % (L(x_min), x_min))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Gradient Descent\n",
    "\n",
    "In most supervised ML applications, we will try to learn a pattern from a number of labeled examples. In Batch Gradient Descent, each iteration loops over entire set of examples.\n",
    "\n",
    "So, let's build 1-layer network of Linear Perceptrons to classify Fisher's IRIS dataset (again!). Remember that a Linear Perceptron can only distinguish between two classes. \n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"http://blog.zabarauskas.com/img/perceptron.gif\"></td>\n",
    "        <td><img src=\"http://cmp.felk.cvut.cz/cmp/courses/recognition/Labs/perceptron/images/linear.png\" />        \n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Since there are 3 classes, our mini-network will have 3 Perceptrons. We'll channel the output\n",
    "of each Perceptron $w_i^T + b$ into a softmax function to pick the final label.  We'll train this network using Batch Gradient Descent.\n",
    "\n",
    "\n",
    "## Getting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['sepal_length' 'sepal_width' 'petal_length' 'petal_width' 'species']\n",
      "Labels:  ['setosa' 'versicolor' 'virginica']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "0           5.1          3.5           1.4          0.2  setosa\n",
       "1           4.9          3.0           1.4          0.2  setosa\n",
       "2           4.7          3.2           1.3          0.2  setosa\n",
       "3           4.6          3.1           1.5          0.2  setosa\n",
       "4           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "iris_df = sns.load_dataset('iris')\n",
    "print('Columns: %s' % (iris_df.columns.values, ))\n",
    "print('Labels:  %s' % (pd.unique(iris_df['species']), ))\n",
    "\n",
    "iris_df.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Softmax Function\n",
    "\n",
    "The softmax function is a technique to apply a probabilistic classifier by making a probability distribution out of a set of values $(v_1, v_2, ..., v_n)$ which may or may not satisfy all the features of probability distribution: \n",
    "\n",
    "- $v_i >= 0$\n",
    "- $\\sum_{i=1}^n v_i = 1$\n",
    "\n",
    "The probability distribution is the Gibbs Distribution: $v'_i = \\frac {\\exp {v_i}} {\\sum_{j=1}^n\\exp {v_j})}$ for $i = 1, 2, ... n$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax([ -500.9  2000.      7.     11.     12.    -15.    100. ]) = [ 0.  1.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    # Uncomment to find out why we shouldn't do it this way...\n",
    "    # return np.exp(x) / np.sum(np.exp(x))\n",
    "    scaled_x = x - np.max(x)    \n",
    "    result = np.exp(scaled_x) / np.sum(np.exp(scaled_x))\n",
    "    return result\n",
    "\n",
    "a = np.array([-500.9, 2000, 7, 11, 12, -15, 100])\n",
    "sm_a = softmax(a)\n",
    "print('Softmax(%s) = %s' % (a, sm_a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-linear Perceptron With SoftMax\n",
    "\n",
    "With softmax, we typically use the cross-entropy error as the function to minimize.\n",
    "\n",
    "The Cross Entropy Error for a given input $X = (x_1, x_2, ..., x_n)$, where each $x_i$ is a vector, is given by:\n",
    "\n",
    "$L(x) = - \\frac {1}{n} \\sum_{i=1}^n y_i^T log(\\hat{y_i})$\n",
    "\n",
    "Where\n",
    "\n",
    "- The sum runs over $X = (x_1, x_2, ..., x_n)$.\n",
    "- Each $y_i$ is the 1-of-n encoded label of the $i$-th example, so it's also a vector. For example, if the labels in order are ('apple', 'banana', 'orange') and the label of $x_i$ is 'banana', then $y_i = [0, 1, 0]$.\n",
    "- $\\hat{y_i}$ is the softmax output for $x_i$ from the network.\n",
    "- The term $y_i^T log(\\hat{y_i})$ is the vector dot product between $y_i$ and  $log(\\hat{y_i})$.\n",
    "\n",
    "## One of n Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode_1_of_n(ordered_labels, y):\n",
    "    label2idx = dict((label, idx)\n",
    "                     for idx, label in enumerate(ordered_labels))\n",
    "    \n",
    "    def encode_one(y_i):        \n",
    "        enc = np.zeros(len(ordered_labels))\n",
    "        enc[label2idx[y_i]] = 1.0\n",
    "        return enc\n",
    "    \n",
    "    return np.array([x for x in map(encode_one, y)])\n",
    "\n",
    "encode_1_of_n(['apple', 'banana', 'orange'], \n",
    "              ['apple', 'banana', 'orange', 'apple', 'apple'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0]\n",
      " [0 1 0]]\n",
      "[[ 2.71828183  1.          1.        ]\n",
      " [ 1.          2.71828183  1.        ]]\n",
      "-1.0\n",
      "\n",
      "[[ 1.          1.          1.        ]\n",
      " [ 1.          2.71828183  1.        ]]\n",
      "-0.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def cross_entropy_loss(Y, Y_hat):\n",
    "    entropy_sum = 0.0    \n",
    "    log_Y_hat = np.log(Y_hat)\n",
    "    \n",
    "    for y, y_hat in zip(Y, log_Y_hat):        \n",
    "        entropy_sum += np.dot(y, y_hat)  \n",
    "    \n",
    "    return -entropy_sum/Y.shape[0]\n",
    "\n",
    "Y_tst = np.array([[1, 0, 0], \n",
    "                  [0, 1, 0]])\n",
    "\n",
    "# log(Y_hat_tst1) is the same as Y_tst, so we expect the x-entropy error to be the min (-1) in this case.\n",
    "print(Y_tst)\n",
    "Y_hat_tst1 = np.array([[np.e, 1, 1,],\n",
    "                     [1, np.e, 1]])\n",
    "print(Y_hat_tst1)\n",
    "print(cross_entropy_loss(Y_tst, Y_hat_tst1))\n",
    "print()\n",
    "\n",
    "# expect it to be > -1\n",
    "Y_hat_tst2 = np.array([[1, 1, 1,],\n",
    "                     [1, np.e, 1]])\n",
    "print(Y_hat_tst2)\n",
    "print(cross_entropy_loss(Y_tst, Y_hat_tst2))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient of the Cross Entropy Error\n",
    "\n",
    "The Gradient update step in Gradient Descent when the Loss Function uses Cross Entropy Error is:\n",
    "\n",
    "$w_i^{j+1} = w_i^{j} - \\eta [\\frac {\\partial L} {\\partial w_i}]^{j}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Iteration [1]: weights diff: \n",
      "                0         1         2         3\n",
      "node_id                                        \n",
      "0        0.782285  1.642967  1.951996  0.824500\n",
      "1        0.737190  0.843614  0.218368  0.102663\n",
      "2        0.045095  0.799353  1.733629  0.721837\n",
      "After Iteration [100]: weights diff: \n",
      "                0         1         2             3\n",
      "node_id                                            \n",
      "0        0.006253  0.003468  0.000718  5.145190e-07\n",
      "1        0.070761  0.033834  0.057067  1.906152e-02\n",
      "2        0.077013  0.037302  0.057785  1.906100e-02\n",
      "After Iteration [200]: weights diff: \n",
      "                0         1         2         3\n",
      "node_id                                        \n",
      "0        0.499088  0.340824  0.137474  0.013713\n",
      "1        1.137667  0.626235  0.813392  0.236197\n",
      "2        0.638579  0.285411  0.675918  0.222484\n",
      "After Iteration [300]: weights diff: \n",
      "                0         1         2         3\n",
      "node_id                                        \n",
      "0        0.002622  0.001437  0.000159  0.000021\n",
      "1        0.711888  0.320010  0.503276  0.152849\n",
      "2        0.714511  0.321447  0.503435  0.152828\n",
      "After Iteration [400]: weights diff: \n",
      "                0         1         2         3\n",
      "node_id                                        \n",
      "0        0.000367  0.000222  0.000114  0.000013\n",
      "1        0.750992  0.346665  0.600615  0.230976\n",
      "2        0.750625  0.346443  0.600502  0.230963\n",
      "After Iteration [500]: weights diff: \n",
      "                0         1         2         3\n",
      "node_id                                        \n",
      "0        0.020768  0.010141  0.013506  0.004634\n",
      "1        0.836765  0.428394  0.684118  0.290854\n",
      "2        0.857533  0.438535  0.697624  0.295488\n",
      "After Iteration [600]: weights diff: \n",
      "                0         1         2         3\n",
      "node_id                                        \n",
      "0        0.000029  0.000010  0.000040  0.000013\n",
      "1        0.000036  0.000013  0.000045  0.000014\n",
      "2        0.000007  0.000003  0.000005  0.000002\n",
      "After Iteration [700]: weights diff: \n",
      "                0         1         2         3\n",
      "node_id                                        \n",
      "0        0.288509  0.196437  0.098172  0.012263\n",
      "1        0.405861  0.315054  0.136197  0.171486\n",
      "2        0.117352  0.118617  0.038026  0.159223\n",
      "After Iteration [800]: weights diff: \n",
      "                0         1         2         3\n",
      "node_id                                        \n",
      "0        0.000510  0.000282  0.000382  0.000136\n",
      "1        0.211186  0.099889  0.168908  0.046117\n",
      "2        0.210676  0.099607  0.168525  0.045981\n",
      "After Iteration [900]: weights diff: \n",
      "                0         1         2         3\n",
      "node_id                                        \n",
      "0        0.000085  0.000070  0.000014  0.000009\n",
      "1        0.000070  0.000062  0.000002  0.000012\n",
      "2        0.000015  0.000008  0.000012  0.000003\n",
      "After Iteration [1000]: weights diff: \n",
      "                    0             1             2             3\n",
      "node_id                                                        \n",
      "0        7.680948e-08  5.178368e-08  2.473312e-08  3.393186e-09\n",
      "1        1.000203e+00  3.400923e-01  7.201698e-01  2.600466e-01\n",
      "2        1.000203e+00  3.400924e-01  7.201698e-01  2.600466e-01\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>expected_label</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   expected_label predicted_label\n",
       "0          setosa          setosa\n",
       "1          setosa          setosa\n",
       "2          setosa          setosa\n",
       "3          setosa          setosa\n",
       "4          setosa          setosa\n",
       "5          setosa          setosa\n",
       "6          setosa          setosa\n",
       "7          setosa          setosa\n",
       "8          setosa          setosa\n",
       "9          setosa          setosa\n",
       "10     versicolor      versicolor\n",
       "11     versicolor      versicolor\n",
       "12     versicolor      versicolor\n",
       "13     versicolor      versicolor\n",
       "14     versicolor      versicolor\n",
       "15     versicolor      versicolor\n",
       "16     versicolor      versicolor\n",
       "17     versicolor      versicolor\n",
       "18     versicolor      versicolor\n",
       "19     versicolor      versicolor\n",
       "20      virginica       virginica\n",
       "21      virginica       virginica\n",
       "22      virginica       virginica\n",
       "23      virginica       virginica\n",
       "24      virginica       virginica\n",
       "25      virginica       virginica\n",
       "26      virginica       virginica\n",
       "27      virginica       virginica\n",
       "28      virginica       virginica\n",
       "29      virginica       virginica"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "class OneLayerNetworkWithSoftMax:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.w, self.bias = None, 0.0\n",
    "        self.optimiser = None\n",
    "        self.output = None\n",
    "        \n",
    "    def init_weights(self, X, Y):\n",
    "        \"\"\"\n",
    "        Initialize a 2D weight matrix as a Dataframe with \n",
    "        dim(n_labels*n_features).         \n",
    "        \"\"\"\n",
    "        self.labels = np.unique(Y)\n",
    "              \n",
    "        w_init = np.random.randn(len(self.labels), X.shape[1])        \n",
    "        self.w = pd.DataFrame(data=w_init)\n",
    "        self.w.index.name = 'node_id'\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Return the predicted label of x using current weights.\n",
    "        \"\"\"\n",
    "        output = self.forward(x, update=False)\n",
    "        max_label_idx = np.argmax(output)\n",
    "        return self.labels[max_label_idx]\n",
    "\n",
    "    def forward(self, x, update=True):\n",
    "        \"\"\"\n",
    "        Calculate softmax(w^Tx+b) for x using current $w_i$ s.\n",
    "        \"\"\"\n",
    "        #output = self.w.apply(lambda row: np.dot(row, x), axis=1)\n",
    "        output = np.dot(self.w, x)\n",
    "        output += self.bias\n",
    "        \n",
    "        output = softmax(output) \n",
    "        if update:\n",
    "            self.output = output        \n",
    "        return output\n",
    "    \n",
    "    def backward(self, x, y, learning_rate):\n",
    "        \"\"\"\n",
    "        Executes the weight update step\n",
    "        \n",
    "            grad = (self.output - y)          \n",
    "            \n",
    "            for i in range(len(grad)):\n",
    "                dw[i] -= grad[i] * x\n",
    "                \n",
    "            w -= learning_rate * dw\n",
    "        \n",
    "        :param x: one sample vector.\n",
    "        :param y: One-hot encoded label for x.\n",
    "        \"\"\"\n",
    "        \n",
    "        # [y_hat1 - y1, y_hat2-y2, ... ]\n",
    "        y_hat_min_y = self.output - y\n",
    "        \n",
    "        # Transpose the above to a column vector\n",
    "        # and then multiply x with each element\n",
    "        # to produce a 2D array (n_labels*n_features), same as w\n",
    "        error_grad = np.apply_along_axis(lambda z: z*x , \n",
    "            1, np.atleast_2d(y_hat_min_y).T)\n",
    "        self.w -= learning_rate * error_grad       \n",
    "    \n",
    "    def print_weight_diff(self, i, w_old, diff_only=True):\n",
    "        if not diff_only:        \n",
    "            print('Before Iteration [%s]: weights are: \\n%s' % \n",
    "                  (i+1, w_old))\n",
    "\n",
    "            print('After Iteration [%s]: weights are: \\n%s' % \n",
    "                  (i+1, self.w))\n",
    "        \n",
    "        w_diff = np.abs(w_old - self.w)\n",
    "        print('After Iteration [%s]: weights diff: \\n%s' % \n",
    "              (i+1, w_diff))\n",
    "                \n",
    "    def train(self, X, Y, \n",
    "              n_iters=1000, \n",
    "              learning_rate=0.2,\n",
    "              minibatch_size=20,\n",
    "              epsilon=1E-8):\n",
    "        \"\"\"\n",
    "        Entry point for the Minibatch SGD training method.\n",
    "                      \n",
    "        Calls forward+backward for each (x_i, y_i) pair and adjusts the\n",
    "        weight w accordingly.        \n",
    "        \"\"\"\n",
    "        print_every = n_iters/10\n",
    "        \n",
    "        self.init_weights(X, Y)    \n",
    "        Y = encode_1_of_n(self.labels, Y)\n",
    "        \n",
    "        n_samples = X.shape[0]     \n",
    "               \n",
    "        # MiniBatch SGD\n",
    "        for i in range(n_iters):\n",
    "            batch_indices = np.random.randint(0, \n",
    "                n_samples-1, minibatch_size)\n",
    "            \n",
    "            X_batch = X[batch_indices, :]\n",
    "            Y_batch = Y[batch_indices, :]\n",
    "            \n",
    "            w_old = self.w.copy()\n",
    "            \n",
    "            for x, y in zip(X_batch, Y_batch):\n",
    "                self.forward(x)\n",
    "                self.backward(x, y, learning_rate)\n",
    "            \n",
    "            if (i == 0) or ((i+1) % print_every == 0):\n",
    "                self.print_weight_diff(i, w_old)          \n",
    "                \n",
    "                \n",
    "# Set aside test data\n",
    "label_grouper = iris_df.groupby('species')\n",
    "test = label_grouper.head(10).set_index('species')\n",
    "train = label_grouper.tail(100).set_index('species')\n",
    "\n",
    "# Train the Network\n",
    "X_train, Y_train = train.as_matrix(), train.index.values\n",
    "nn = OneLayerNetworkWithSoftMax()\n",
    "nn.train(X_train, Y_train)\n",
    "\n",
    "# Test\n",
    "results = test.apply(lambda row : nn.predict(row.as_matrix()), axis=1)\n",
    "results.name = 'predicted_label'\n",
    "results.index.name = 'expected_label'\n",
    "\n",
    "results.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient of the Cross Entropy Error\n",
    "\n",
    "**Recap** We know the the cross entropy error is the average of the vector products between the 1-hot enconding of label and the softmax output.\n",
    "\n",
    "$L = - \\frac {1}{n} \\sum_{i=1}^n Y_i^T ln(\\hat Y_i)$\n",
    "\n",
    "Where the sum runs over all of the $n$ input samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a complex derivation, and we need to approach it step-by step. First, let's work out what the $i$-th sample contributes to the gradient of L, i.e. the derivative of - $Y_i^Tln(\\hat Y_i)$.\n",
    "\n",
    "Let's draw the structure of the Network using networkx for a 2-class problem, so we have 2 input nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('i', 'n1'): <matplotlib.text.Text at 0x294ab87dda0>,\n",
       " ('i', 'n2'): <matplotlib.text.Text at 0x294ab888470>,\n",
       " ('n1', 's1'): <matplotlib.text.Text at 0x294ab87d630>,\n",
       " ('n1', 's2'): <matplotlib.text.Text at 0x294ab882cc0>,\n",
       " ('n2', 's1'): <matplotlib.text.Text at 0x294ab53ef60>,\n",
       " ('n2', 's2'): <matplotlib.text.Text at 0x294ab888be0>,\n",
       " ('s1', 'y1'): <matplotlib.text.Text at 0x294ab882550>,\n",
       " ('s2', 'y2'): <matplotlib.text.Text at 0x294ab88d390>}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnAAAAGuCAYAAAAQzzthAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRuAAAGYEDgBgJoR40Slw4t5dAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x294ab4296d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from matplotlib import pylab\n",
    "\n",
    "G = nx.DiGraph()\n",
    "G.add_edges_from(\n",
    "    [('i', 'n1'), \n",
    "     ('i', 'n2'),\n",
    "     ('n1', 's1'),\n",
    "     ('n2', 's1'),\n",
    "     ('n1', 's2'),\n",
    "     ('n2', 's2'),\n",
    "     ('s1', 'y1'),\n",
    "     ('s2', 'y2'),\n",
    "    ])\n",
    "\n",
    "pos = {'i': (1, 1), \n",
    "       'n1': (2, 0), 'n2': (2, 2),\n",
    "       's1': (3, 0), 's2': (3, 2),\n",
    "       'y1': (4, 0), 'y2': (4, 2),\n",
    "      }\n",
    "\n",
    "labels = {'i': r'$x_i$',\n",
    "         'n1': r'$w_1$', 'n2': r'$w_2$',\n",
    "         's1': r'$s_1$', # r'$\\frac {\\exp(z_{i1})} {S_i}$', \n",
    "         's2': r'$s_2$', # r'$\\frac {\\exp(z_{i2})} {S_i}$'         \n",
    "         }\n",
    "\n",
    "edge_labels = {('i', 'n1'): r'$x_i$', \n",
    "               ('i', 'n2'): r'$x_i$',\n",
    "               ('n1', 's1'): r'$w_1^Tx_i$',\n",
    "               ('n1', 's2'): r'$w_1^Tx_i$', \n",
    "               ('n2', 's1'): r'$w_2^Tx_i$', \n",
    "               ('n2', 's2'): r'$w_2^Tx_i$',\n",
    "               ('n2', 's1'): r'$w_2^Tx_i$',\n",
    "               ('s1', 'y1'): r'$\\frac {\\exp(z_{i1})} {S_i}$',\n",
    "               ('s2', 'y2'): r'$\\frac {\\exp(z_{i2})} {S_i}$',\n",
    "              }\n",
    "nx.draw(G, pos=pos, node_size=1000)\n",
    "nx.draw_networkx_labels(G,pos,labels, font_size=15, color='white')\n",
    "nx.draw_networkx_edge_labels(G, pos=pos, \n",
    "    edge_labels=edge_labels, font_size=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the derivative $- Y_i^Tln(\\hat {Y_i})$, we will break down the vector sum:\n",
    "\n",
    "$L_i = - [y_1 ln (\\hat {y_1}) + y_2 ln (\\hat {y_2}) + ... ]$, where \n",
    "\n",
    "* Each of $(y_1, y_2, ...)$ is an element of the one hot encoded label for sample $x_i$, so only one of them is 1, all the others are 0.\n",
    "* Each of $(\\hat {y_1}, \\hat {y_2}, ...)$ is an element of the softmax output for input $x_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that\n",
    "\n",
    "$\\begin{equation}\n",
    "y_1 ln (\\hat {y_1}) = y_1 ln \\frac {\\exp(z_{i1})} {\\exp(z_{i1}) + \\exp(z_{i2}) + ...} \\\\\n",
    "y_2 ln (\\hat {y_2}) = y_2 ln \\frac {\\exp(z_{i2})} {\\exp(z_{i1}) + \\exp(z_{i2}) + ...} \\\\\n",
    "\\vdots\n",
    "\\end{equation}$\n",
    "\n",
    "Where $z_{i1} = w_1^Tx_i, z_{i2} = w_2^Tx_i$, and so on.\n",
    "\n",
    "Our end goal is to calculate $(\\frac {\\partial L_i}{\\partial w_1}, \\frac {\\partial L_i}{\\partial w_2}, ...)$. We can use the Chain rule to produce:\n",
    "\n",
    "$\\begin{equation}\n",
    "\\frac {\\partial L_i}{\\partial w_1} = \\frac {\\partial L_i} {\\partial z_{i1}} \\frac {\\partial z_{i1}}{\\partial w_1} \\\\\n",
    "\\frac {\\partial L_i}{\\partial w_2} = \\frac {\\partial L_i} {\\partial z_{i2}} \\frac {\\partial z_{i2}}{\\partial w_2} \\\\\n",
    "\\vdots\n",
    "\\end{equation}$\n",
    "\n",
    "The denominator is the same for all of $(\\hat {y_1}, \\hat {y_2}, ...)$, so let's call that $S_i$. \n",
    "\n",
    "$S_i = \\exp(z_{i1}) + \\exp(z_{i1}) + ...$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the equations above become:\n",
    "\n",
    "$\\begin{equation}\n",
    "y_1 ln(\\hat {y_1}) = z_{i1} - ln(S_i) \\\\\n",
    "y_2 ln(\\hat {y_2}) = z_{i2} - ln(S_i) \\\\\n",
    "\\vdots\n",
    "\\end{equation}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the partial derivative of all these equations w.r.t $z_{i1}$, we get:\n",
    "\n",
    "$\\begin{equation}\n",
    "\\frac {y_1} {\\hat {y_1}} \\frac {\\partial \\hat {y_1}} {\\partial z_{i1}} = y_1(1 - \\frac {z_{i1}} {S_i}) = y_1(1 - \\hat {y_1}) \\\\\n",
    "\\frac {y_2} {\\hat {y_2}} \\frac {\\partial \\hat {y_2}} {\\partial z_{i1}} =  - y_2 \\frac {z_{i1}} {S_i} = - y_2 \\hat {y_1} \\\\\n",
    "\\vdots\n",
    "\\end{equation}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we can express $\\frac {\\partial L_i} {\\partial z_{j1}}$ as:\n",
    "\n",
    "$\\frac {\\partial L_i} {\\partial z_{j1}} = [y1(\\hat {y_1} - 1) + y2 \\hat {y_1} + y3 \\hat{y_1}+ ...] = [\\hat {y_1}(y_1 + y_2 + ...) - y_1] = (\\hat {y_1} - y_1)$\n",
    "\n",
    "Since exactly 1 of $(y_1 + y_2 + ...)$ is 1, and all the others are zero.\n",
    "\n",
    "Similarly, we can prove:\n",
    "\n",
    "$\\begin{equation}\n",
    "\\frac {\\partial L_i} {\\partial z_{j2}} = (\\hat {y_2} - y_2) \\\\\n",
    "\\frac {\\partial L_i} {\\partial z_{j3}} = (\\hat {y_3} - y_3) \\\\\n",
    "\\vdots\n",
    "\\end{equation}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noting that \n",
    "\n",
    "$\\begin{equation}\n",
    "\\frac {\\partial z_{i1}} {\\partial w_1} = x_i \\\\\n",
    "\\frac {\\partial z_{i2}} {\\partial w_2} = x_i \\\\\n",
    "\\vdots\n",
    "\\end{equation}$\n",
    "\n",
    "We finally arrive at the result:\n",
    "\n",
    "$\\begin{equation}\n",
    "\\frac {\\partial L_i} {\\partial w_1} = - (\\hat {y_1} - y_1)x_i \\\\\n",
    "\\frac {\\partial L_i} {\\partial w_2} = - (\\hat {y_2} - y_2)x_i \\\\\n",
    "\\vdots\n",
    "\\end{equation}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
