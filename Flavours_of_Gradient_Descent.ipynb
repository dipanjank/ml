{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flavours of Gradient Descent\n",
    "\n",
    "A quick recap of the Gradient Descent method: This is an iterative algorithm to minize a loss function $L(x)$, where we start with a guess of what the answer should be - and then take steps proportional to the gradient at the current point.\n",
    "\n",
    "$x = x_0$ (initial guess)\n",
    "\n",
    "Until Convergence is achieved:\n",
    "    \n",
    "$x_{i+1} = x_{i} - \\eta\\nabla_L(x_i)$\n",
    "\n",
    "For example, Let's say $L(x) = x^2 - 2x + 1$ and we start at $x0 = 2$. Coding the Gradient Descent method in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum value of L(x) = x**2 - 2*x + 1.0 is [0.00] at x = [1.00]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def L(x):\n",
    "    return x**2 - 2*x + 1\n",
    "\n",
    "def L_prime(x):\n",
    "    return 2*x - 2\n",
    "\n",
    "\n",
    "def converged(x_prev, x, epsilon):\n",
    "    \"Return True if the abs value of all elements in x-x_prev are <= epsilon.\"\n",
    "    \n",
    "    absdiff = np.abs(x-x_prev)\n",
    "    return np.all(absdiff <= epsilon)\n",
    "\n",
    "\n",
    "def gradient_descent(f_prime, x_0, learning_rate=0.2, n_iters=100, epsilon=1E-8):\n",
    "    x = x_0\n",
    "    \n",
    "    for _ in range(n_iters):\n",
    "        x_prev = x\n",
    "        x -= learning_rate*f_prime(x)\n",
    "        \n",
    "        if converged(x_prev, x, epsilon):\n",
    "            break\n",
    "            \n",
    "    return x\n",
    "\n",
    "x_min = gradient_descent(L_prime, 2)\n",
    "\n",
    "print('Minimum value of L(x) = x**2 - 2*x + 1.0 is [%.2f] at x = [%.2f]' % (L(x_min), x_min))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Gradient Descent\n",
    "\n",
    "In most supervised ML applications, we will try to learn a pattern from a number of labeled examples. In Batch Gradient Descent, each iteration loops over entire set of examples.\n",
    "\n",
    "So, let's build 1-layer network of Linear Perceptrons to classify Fisher's IRIS dataset (again!). Remember that a Linear Perceptron can only distinguish between two classes. \n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"http://blog.zabarauskas.com/img/perceptron.gif\"></td>\n",
    "        <td><img src=\"http://cmp.felk.cvut.cz/cmp/courses/recognition/Labs/perceptron/images/linear.png\" />        \n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Since there are 3 classes, our mini-network will have 3 Perceptrons. We'll channel the output\n",
    "of each Perceptron $w_i^T + b$ into a softmax function to pick the final label.  We'll train this network using Batch Gradient Descent.\n",
    "\n",
    "\n",
    "## Getting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sepal_length' 'sepal_width' 'petal_length' 'petal_width' 'species']\n",
      "['setosa' 'versicolor' 'virginica']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "0           5.1          3.5           1.4          0.2  setosa\n",
       "1           4.9          3.0           1.4          0.2  setosa\n",
       "2           4.7          3.2           1.3          0.2  setosa\n",
       "3           4.6          3.1           1.5          0.2  setosa\n",
       "4           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "iris_df = sns.load_dataset('iris')\n",
    "print(iris_df.columns.values)\n",
    "print(pd.unique(iris_df['species']))\n",
    "\n",
    "iris_df.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Softmax Function\n",
    "\n",
    "The softmax function is a technique to apply a probabilistic classifier by making a probability distribution out of a set of values $(v_1, v_2, ..., v_n)$ which may or may not satisfy all the features of probability distribution: \n",
    "\n",
    "- $v_i >= 0$\n",
    "- $\\sum_{i=1}^n v_i = 1$\n",
    "\n",
    "The probability distribution is the Gibbs Distribution: $v'_i = \\frac {\\exp {v_i}} {\\sum_{j=1}^n\\exp {v_j})}$ for $i = 1, 2, ... n$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log of softmax vector of [  -5.9    2.     7.    11.    12.   -15.   100. ] is [ 483.   490.9  495.9  499.9  500.9  473.9  588.9]\n"
     ]
    }
   ],
   "source": [
    "def softmax(x, log=True):\n",
    "    scaled_x = x - np.max(x)\n",
    "    \n",
    "    if log:\n",
    "        result = scaled_x - np.sum(scaled_x)\n",
    "    else:\n",
    "        result = np.exp(scaled_x) / np.sum(np.exp(scaled_x))    \n",
    "    return result\n",
    "\n",
    "a = np.array([-5.9, 2, 7, 11, 12, -15, 100])\n",
    "sm_a = softmax(a)\n",
    "print('log of softmax vector of %s is %s' % (a, sm_a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-linear Perceptron With SoftMax\n",
    "\n",
    "With softmax, it doesn't make sense to minimize the mean-square loss. Instead we'll use the cross-entropy loss.\n",
    "\n",
    "The Cross Entropy Error for a given input vector $x$ is given by:\n",
    "\n",
    "$L(x) = - \\frac {1}{n} \\sum_{i=1}^n y_i log(\\hat{y_i})$\n",
    "\n",
    "Where\n",
    "\n",
    "- The sum runs over the set of all input examples.\n",
    "- Each $y_i$ is the 1-of-n encoded label of the $i$-th example. For example, if the labels in order are ('apple', 'banana', 'orange') and the label of $x_i$ is 'banana', then $y_i = [0, 1, 0]$.\n",
    "- $\\hat{y_i}$ is the softmax output for $x_i$ from the network.\n",
    "- The term $y_i log(\\hat{y_i})$ is the vector dot product between $y_i$ and  $log(\\hat{y_i})$.\n",
    "\n",
    "## One of n Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode_1_of_n(ordered_labels, y):\n",
    "    label2idx = dict((label, idx)\n",
    "                     for idx, label in enumerate(ordered_labels))\n",
    "    \n",
    "    def encode_one(y_i):        \n",
    "        enc = np.zeros(len(ordered_labels))\n",
    "        enc[label2idx[y_i]] = 1.0\n",
    "        return enc\n",
    "    \n",
    "    return np.array([x for x in map(encode_one, y)])\n",
    "\n",
    "encode_1_of_n(['apple', 'banana', 'orange'], \n",
    "              ['apple', 'banana', 'orange', 'apple', 'apple'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]]\n",
      "[[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cross_entropy_loss(Y, Y_hat):\n",
    "    entropy_sum = 0.0\n",
    "    \n",
    "    for y, y_hat in zip(Y, Y_hat):       \n",
    "        entropy_sum += np.dot(y, y_hat)      \n",
    "    \n",
    "    return -entropy_sum/Y.shape[0]\n",
    "\n",
    "k1, k2 = 1, np.e\n",
    "Y_temp = np.array([[k2, k1, k1],[k1,k2, k1], [k1, k1, k2]])\n",
    "Y_hat_tst = np.log(Y_temp)\n",
    "print(Y_hat_tst)\n",
    "\n",
    "Y_tst = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n",
    "print(Y_tst)\n",
    "cross_entropy_loss(Y_tst, Y_hat_tst)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient of the Cross Entropy Error\n",
    "\n",
    "To run Gradient Descent on the Cross Entropy Error function, we have to calculate it's derivative w.r.t. the weight vector $w$.\n",
    "\n",
    "$L(x) = - \\frac {1}{n} \\sum_{i=1}^n y_i log(\\hat{y_i})$\n",
    "\n",
    "Substituting the expression for $\\hat{y_i}$ in terms of $w$ and $x$, we get:\n",
    "\n",
    "$Y_i = y_i log(\\hat{y_i}) = y_i log [\\frac {exp(w_i^Tx+b)} {\\sum_{j=1}^l exp(w_j^Tx+b)}]$\n",
    "\n",
    "Define $\\sigma(w_i) = \\frac {exp(w_i^Tx+b)} {\\sum_{j=1}^l exp(w_j^Tx+b)}$. Then we have the identity \n",
    "\n",
    "$\\frac {\\partial\\sigma} {\\partial w_i} = \\sigma(w_i)^T(1-\\sigma(w_i))$.\n",
    "\n",
    "Now, $\\frac {\\partial Y_i}{\\partial w_i} = \\frac {\\partial}{\\partial w_i} [y_i log \\sigma(w_i)]$. Applying the Chain Rule, we get:\n",
    "\n",
    "$\\frac {\\partial Y_i}{\\partial w_i} = y_i \\frac {1} {\\sigma(w_i)} \\frac {\\partial \\sigma} {\\partial w_i} = y_i(1-\\sigma(w_i))$.\n",
    "\n",
    "So the derivative of the Cross Entropy Error becomes:\n",
    "\n",
    "$\\frac {\\partial L} {\\partial w_i} = - \\frac {1}{n} \\frac {\\partial} {\\partial w_i} [\\sum_{i=1}^n Y_i] = - \\frac {1}{n} y_i(1-\\sigma(w_i))$\n",
    "\n",
    "Since all the terms in the sum except i-th term are treated as constant in taking the partial dervative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the Gradient update step in Gradient Descent when the Loss Function uses Cross Entropy Error is:\n",
    "\n",
    "$w_i^{j+1} = w_i^{j} - \\eta [\\frac {\\partial L} {\\partial w_i}]^{j}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Iteration [1]: weights are:\n",
      "(array([-1.05795999, -1.18502988, -0.92504861, -0.22992313]),)\n",
      "(array([-0.25611782, -0.33042852, -0.8521412 ,  2.72674516]),)\n",
      "(array([ 1.01281817, -0.2151042 , -0.25658776, -0.25663085]),)\n",
      "After Iteration [1]: weights are:\n",
      "[[-1.45025426 -1.57732414 -1.31734287 -0.62221739]]\n",
      "[[-1.74929994 -1.82361063 -2.34532331  1.23356305]]\n",
      "[[-0.49867172 -1.72659409 -1.76807765 -1.76812074]]\n",
      "After Iteration [1]: weights diff:\n",
      "[[ 0.39229426  0.39229426  0.39229426  0.39229426]]\n",
      "[[ 1.49318211  1.49318211  1.49318211  1.49318211]]\n",
      "[[ 1.51148989  1.51148989  1.51148989  1.51148989]]\n",
      "Before Iteration [100]: weights are:\n",
      "[[-23.22550658 -23.35257647 -23.0925952  -22.39746972]]\n",
      "[[-23.33228549 -23.40659618 -23.92830887 -20.34942251]]\n",
      "[[-22.31870528 -23.54662765 -23.5881112  -23.58815429]]\n",
      "After Iteration [100]: weights are:\n",
      "[[-23.4451463  -23.57221618 -23.31223491 -22.61710943]]\n",
      "[[-23.5519252  -23.6262359  -24.14794858 -20.56906222]]\n",
      "[[-22.53834499 -23.76626736 -23.80775091 -23.80779401]]\n",
      "After Iteration [100]: weights diff:\n",
      "[[ 0.21963971  0.21963971  0.21963971  0.21963971]]\n",
      "[[ 0.21963971  0.21963971  0.21963971  0.21963971]]\n",
      "[[ 0.21963971  0.21963971  0.21963971  0.21963971]]\n",
      "Before Iteration [200]: weights are:\n",
      "[[-45.18947778 -45.31654767 -45.0565664  -44.36144092]]\n",
      "[[-45.29625669 -45.37056738 -45.89228006 -42.31339371]]\n",
      "[[-44.28267647 -45.51059884 -45.5520824  -45.55212549]]\n",
      "After Iteration [200]: weights are:\n",
      "[[-45.40911749 -45.53618738 -45.27620611 -44.58108063]]\n",
      "[[-45.5158964  -45.59020709 -46.11191978 -42.53303342]]\n",
      "[[-44.50231619 -45.73023856 -45.77172211 -45.7717652 ]]\n",
      "After Iteration [200]: weights diff:\n",
      "[[ 0.21963971  0.21963971  0.21963971  0.21963971]]\n",
      "[[ 0.21963971  0.21963971  0.21963971  0.21963971]]\n",
      "[[ 0.21963971  0.21963971  0.21963971  0.21963971]]\n",
      "Before Iteration [300]: weights are:\n",
      "[[-67.15344898 -67.28051887 -67.0205376  -66.32541212]]\n",
      "[[-67.26022789 -67.33453858 -67.85625126 -64.27736491]]\n",
      "[[-66.24664767 -67.47457004 -67.5160536  -67.51609669]]\n",
      "After Iteration [300]: weights are:\n",
      "[[-67.37308869 -67.50015858 -67.24017731 -66.54505183]]\n",
      "[[-67.4798676  -67.55417829 -68.07589098 -64.49700462]]\n",
      "[[-66.46628738 -67.69420976 -67.73569331 -67.7357364 ]]\n",
      "After Iteration [300]: weights diff:\n",
      "[[ 0.21963971  0.21963971  0.21963971  0.21963971]]\n",
      "[[ 0.21963971  0.21963971  0.21963971  0.21963971]]\n",
      "[[ 0.21963971  0.21963971  0.21963971  0.21963971]]\n",
      "Before Iteration [400]: weights are:\n",
      "[[-89.11742018 -89.24449007 -88.9845088  -88.28938332]]\n",
      "[[-89.22419909 -89.29850978 -89.82022246 -86.24133611]]\n",
      "[[-88.21061887 -89.43854124 -89.4800248  -89.48006789]]\n",
      "After Iteration [400]: weights are:\n",
      "[[-89.33705989 -89.46412978 -89.20414851 -88.50902303]]\n",
      "[[-89.4438388  -89.51814949 -90.03986217 -86.46097582]]\n",
      "[[-88.43025858 -89.65818095 -89.69966451 -89.6997076 ]]\n",
      "After Iteration [400]: weights diff:\n",
      "[[ 0.21963971  0.21963971  0.21963971  0.21963971]]\n",
      "[[ 0.21963971  0.21963971  0.21963971  0.21963971]]\n",
      "[[ 0.21963971  0.21963971  0.21963971  0.21963971]]\n",
      "Before Iteration [500]: weights are:\n",
      "[[-111.08139138 -111.20846127 -110.94848    -110.25335452]]\n",
      "[[-111.18817029 -111.26248098 -111.78419366 -108.20530731]]\n",
      "[[-110.17459007 -111.40251244 -111.443996   -111.44403909]]\n",
      "After Iteration [500]: weights are:\n",
      "[[-111.30103109 -111.42810098 -111.16811971 -110.47299423]]\n",
      "[[-111.40781    -111.48212069 -112.00383337 -108.42494702]]\n",
      "[[-110.39422978 -111.62215215 -111.66363571 -111.6636788 ]]\n",
      "After Iteration [500]: weights diff:\n",
      "[[ 0.21963971  0.21963971  0.21963971  0.21963971]]\n",
      "[[ 0.21963971  0.21963971  0.21963971  0.21963971]]\n",
      "[[ 0.21963971  0.21963971  0.21963971  0.21963971]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "class OneLayerNetworkWithSoftMax:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.w, self.bias = None, 0.0\n",
    "        self.optimiser = None\n",
    "        self.output = None\n",
    "        \n",
    "    def init_weights(self, X, Y):\n",
    "        \"\"\"\n",
    "        Initialize a 2D weight matrix represented by a pd.Series.        \n",
    "        The DataFrame is indexed by the node name (this is NOT the label) and one column - \n",
    "        each row element is a numpy array.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.labels = np.unique(Y)\n",
    "        w_init = [(np.random.randn(X.shape[1]),) for _ in self.labels]\n",
    "        self.w = pd.Series(data=w_init, name='weight')\n",
    "        self.w.index.name = 'node_id'\n",
    "\n",
    "    def forward(self, x, update=False, log=True):\n",
    "        \"\"\"\n",
    "        Calculate softmax(w^Tx+b) for current weight across all nodes. \n",
    "        Use that to find the label which has the max probability.\n",
    "        \"\"\"\n",
    "        output = self.w.map(lambda row: np.dot(row, x))\n",
    "        output += self.bias       \n",
    "        \n",
    "        # Produce the log-linear softmax output vector.\n",
    "        output = softmax(output.as_matrix()) \n",
    "        if update:\n",
    "            self.output = output        \n",
    "        return output\n",
    "    \n",
    "    def backward(self, x, y, n_samples, learning_rate):\n",
    "        \"\"\"\n",
    "        Executes the weight update step.\n",
    "        \n",
    "        :param x: one sample vector.\n",
    "        :param y: One-hot encoded label for x.\n",
    "        \"\"\"\n",
    "        sigma = self.forward(x, update=False, log=False)\n",
    "        error_grad = (1/n_samples) * y *(sigma-1)\n",
    "        w = self.w - learning_rate * error_grad\n",
    "        self.w = w\n",
    "    \n",
    "    def print_weight_diff(self, i, w_old):\n",
    "        print('Before Iteration [%s]: weights are:' % (i+1,))\n",
    "        for w in w_old:\n",
    "            print(w)\n",
    "        \n",
    "        w_new = self.w\n",
    "        print('After Iteration [%s]: weights are:' % (i+1,))\n",
    "        for w in w_new:\n",
    "            print(w)\n",
    "              \n",
    "        w_diff = np.abs(w_old-w_new)\n",
    "        print('After Iteration [%s]: weights diff:' % (i+1, ))\n",
    "        for w in w_diff:\n",
    "            print(w)\n",
    "\n",
    "        \n",
    "    def train(self, X, Y, n_iters=500, learning_rate=0.2, epsilon=1E-8):\n",
    "        \"\"\"\n",
    "        Entry point for the training method. Repeatedly calls forward+backward.\n",
    "        \n",
    "        \"\"\"\n",
    "        print_every = 100\n",
    "        \n",
    "        self.init_weights(X, Y)    \n",
    "        Y = encode_1_of_n(self.labels, Y)\n",
    "        \n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        # Batch Gradient Descent\n",
    "        for i in range(n_iters):\n",
    "            w_old = self.w.copy()\n",
    "            \n",
    "            for x, y in zip(X, Y):\n",
    "                self.forward(x, update=True, log=True)\n",
    "                self.backward(x, y, n_samples, learning_rate)\n",
    "            \n",
    "            if (i == 0) or ((i+1) % print_every == 0):\n",
    "                self.print_weight_diff(i, w_old)          \n",
    "                \n",
    "                \n",
    "indexed_iris_df = iris_df.set_index('species')\n",
    "\n",
    "X = indexed_iris_df.as_matrix()\n",
    "Y = indexed_iris_df.index.values\n",
    "\n",
    "nn = OneLayerNetworkWithSoftMax()\n",
    "nn.train(X, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
