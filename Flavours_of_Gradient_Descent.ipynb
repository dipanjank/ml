{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flavours of Gradient Descent\n",
    "\n",
    "A quick recap of the Gradient Descent method: This is an iterative algorithm to minize a loss function $L(x)$, where we start with a guess of what the answer should be - and then take steps proportional to the gradient at the current point.\n",
    "\n",
    "$x = x_0$ (initial guess)\n",
    "\n",
    "Until Convergence is achieved:\n",
    "    \n",
    "$x_{i+1} = x_{i} - \\eta\\nabla_L(x_i)$\n",
    "\n",
    "For example, Let's say $L(x) = x^2 - 2x + 1$ and we start at $x0 = 2$. Coding the Gradient Descent method in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum value of L(x) = x**2 - 2*x + 1.0 is [0.00] at x = [1.00]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def L(x):\n",
    "    return x**2 - 2*x + 1\n",
    "\n",
    "def L_prime(x):\n",
    "    return 2*x - 2\n",
    "\n",
    "\n",
    "def converged(x_prev, x, epsilon):\n",
    "    \"Return True if the abs value of all elements in x-x_prev are <= epsilon.\"\n",
    "    \n",
    "    absdiff = np.abs(x-x_prev)\n",
    "    return np.all(absdiff <= epsilon)\n",
    "\n",
    "\n",
    "def gradient_descent(f_prime, x_0, learning_rate=0.2, n_iters=100, epsilon=1E-8):\n",
    "    x = x_0\n",
    "    \n",
    "    for _ in range(n_iters):\n",
    "        x_prev = x\n",
    "        x -= learning_rate*f_prime(x)\n",
    "        \n",
    "        if converged(x_prev, x, epsilon):\n",
    "            break\n",
    "            \n",
    "    return x\n",
    "\n",
    "x_min = gradient_descent(L_prime, 2)\n",
    "\n",
    "print('Minimum value of L(x) = x**2 - 2*x + 1.0 is [%.2f] at x = [%.2f]' % (L(x_min), x_min))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Gradient Descent\n",
    "\n",
    "In most supervised ML applications, we will try to learn a pattern from a number of labeled examples. In Batch Gradient Descent, each iteration loops over entire set of examples.\n",
    "\n",
    "So, let's build 1-layer network of Linear Perceptrons to classify Fisher's IRIS dataset (again!). Remember that a Linear Perceptron can only distinguish between two classes. \n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"http://blog.zabarauskas.com/img/perceptron.gif\"></td>\n",
    "        <td><img src=\"http://cmp.felk.cvut.cz/cmp/courses/recognition/Labs/perceptron/images/linear.png\" />        \n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Since there are 3 classes, our mini-network will have 3 Perceptrons. We'll channel the output\n",
    "of each Perceptron $w_i^T + b$ into a softmax function to pick the final label.  We'll train this network using Batch Gradient Descent.\n",
    "\n",
    "\n",
    "## Getting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['sepal_length' 'sepal_width' 'petal_length' 'petal_width' 'species']\n",
      "Labels:  ['setosa' 'versicolor' 'virginica']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "0           5.1          3.5           1.4          0.2  setosa\n",
       "1           4.9          3.0           1.4          0.2  setosa\n",
       "2           4.7          3.2           1.3          0.2  setosa\n",
       "3           4.6          3.1           1.5          0.2  setosa\n",
       "4           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "iris_df = sns.load_dataset('iris')\n",
    "print('Columns: %s' % (iris_df.columns.values, ))\n",
    "print('Labels:  %s' % (pd.unique(iris_df['species']), ))\n",
    "\n",
    "iris_df.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Softmax Function\n",
    "\n",
    "The softmax function is a technique to apply a probabilistic classifier by making a probability distribution out of a set of values $(v_1, v_2, ..., v_n)$ which may or may not satisfy all the features of probability distribution: \n",
    "\n",
    "- $v_i >= 0$\n",
    "- $\\sum_{i=1}^n v_i = 1$\n",
    "\n",
    "The probability distribution is the Gibbs Distribution: $v'_i = \\frac {\\exp {v_i}} {\\sum_{j=1}^n\\exp {v_j})}$ for $i = 1, 2, ... n$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax([ -500.9  2000.      7.     11.     12.    -15.    100. ]) = [ 0.  1.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    # Uncomment to find out why we shouldn't do it this way...\n",
    "    # return np.exp(x) / np.sum(np.exp(x))\n",
    "    scaled_x = x - np.max(x)    \n",
    "    result = np.exp(scaled_x) / np.sum(np.exp(scaled_x))\n",
    "    return result\n",
    "\n",
    "a = np.array([-500.9, 2000, 7, 11, 12, -15, 100])\n",
    "sm_a = softmax(a)\n",
    "print('Softmax(%s) = %s' % (a, sm_a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-linear Perceptron With SoftMax\n",
    "\n",
    "With softmax, we typically use the cross-entropy error as the function to minimize.\n",
    "\n",
    "The Cross Entropy Error for a given input $X = (x_1, x_2, ..., x_n)$, where each $x_i$ is a vector, is given by:\n",
    "\n",
    "$L(x) = - \\frac {1}{n} \\sum_{i=1}^n y_i^T log(\\hat{y_i})$\n",
    "\n",
    "Where\n",
    "\n",
    "- The sum runs over $X = (x_1, x_2, ..., x_n)$.\n",
    "- Each $y_i$ is the 1-of-n encoded label of the $i$-th example, so it's also a vector. For example, if the labels in order are ('apple', 'banana', 'orange') and the label of $x_i$ is 'banana', then $y_i = [0, 1, 0]$.\n",
    "- $\\hat{y_i}$ is the softmax output for $x_i$ from the network.\n",
    "- The term $y_i^T log(\\hat{y_i})$ is the vector dot product between $y_i$ and  $log(\\hat{y_i})$.\n",
    "\n",
    "## One of n Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode_1_of_n(ordered_labels, y):\n",
    "    label2idx = dict((label, idx)\n",
    "                     for idx, label in enumerate(ordered_labels))\n",
    "    \n",
    "    def encode_one(y_i):        \n",
    "        enc = np.zeros(len(ordered_labels))\n",
    "        enc[label2idx[y_i]] = 1.0\n",
    "        return enc\n",
    "    \n",
    "    return np.array([x for x in map(encode_one, y)])\n",
    "\n",
    "encode_1_of_n(['apple', 'banana', 'orange'], \n",
    "              ['apple', 'banana', 'orange', 'apple', 'apple'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0]\n",
      " [0 1 0]]\n",
      "[[ 2.71828183  1.          1.        ]\n",
      " [ 1.          2.71828183  1.        ]]\n",
      "-1.0\n",
      "\n",
      "[[ 1.          1.          1.        ]\n",
      " [ 1.          2.71828183  1.        ]]\n",
      "-0.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def cross_entropy_loss(Y, Y_hat):\n",
    "    entropy_sum = 0.0    \n",
    "    log_Y_hat = np.log(Y_hat)\n",
    "    \n",
    "    for y, y_hat in zip(Y, log_Y_hat):        \n",
    "        entropy_sum += np.dot(y, y_hat)  \n",
    "    \n",
    "    return -entropy_sum/Y.shape[0]\n",
    "\n",
    "Y_tst = np.array([[1, 0, 0], \n",
    "                  [0, 1, 0]])\n",
    "\n",
    "# log(Y_hat_tst1) is the same as Y_tst, so we expect the x-entropy error to be the min (-1) in this case.\n",
    "print(Y_tst)\n",
    "Y_hat_tst1 = np.array([[np.e, 1, 1,],\n",
    "                     [1, np.e, 1]])\n",
    "print(Y_hat_tst1)\n",
    "print(cross_entropy_loss(Y_tst, Y_hat_tst1))\n",
    "print()\n",
    "\n",
    "# expect it to be > -1\n",
    "Y_hat_tst2 = np.array([[1, 1, 1,],\n",
    "                     [1, np.e, 1]])\n",
    "print(Y_hat_tst2)\n",
    "print(cross_entropy_loss(Y_tst, Y_hat_tst2))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient of the Cross Entropy Error\n",
    "\n",
    "The Gradient update step in Gradient Descent when the Loss Function uses Cross Entropy Error is:\n",
    "\n",
    "$w_i^{j+1} = w_i^{j} - \\eta [\\frac {\\partial L} {\\partial w_i}]^{j}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Iteration [1]: weights diff: \n",
      "                0         1         2         3\n",
      "node_id                                        \n",
      "0        0.492922  0.323592  0.138847  0.020738\n",
      "1        0.182884  0.177454  0.054493  0.030612\n",
      "2        0.310038  0.146138  0.193340  0.051350\n",
      "After Iteration [1000]: weights diff: \n",
      "                0         1         2         3\n",
      "node_id                                        \n",
      "0        0.000235  0.000683  0.000751  0.000313\n",
      "1        0.027462  0.012721  0.023538  0.007085\n",
      "2        0.027697  0.013404  0.022787  0.006772\n",
      "After Iteration [2000]: weights diff: \n",
      "                0         1         2         3\n",
      "node_id                                        \n",
      "0        0.000021  0.000231  0.000365  0.000134\n",
      "1        0.047297  0.025439  0.039449  0.014066\n",
      "2        0.047275  0.025208  0.039814  0.014201\n",
      "After Iteration [3000]: weights diff: \n",
      "                0         1         2         3\n",
      "node_id                                        \n",
      "0        0.000145  0.000224  0.000146  0.000075\n",
      "1        0.042928  0.021839  0.033308  0.011074\n",
      "2        0.043073  0.022063  0.033162  0.010999\n",
      "After Iteration [4000]: weights diff: \n",
      "                0         1         2         3\n",
      "node_id                                        \n",
      "0        0.000222  0.000029  0.000353  0.000166\n",
      "1        0.011032  0.006324  0.009230  0.003108\n",
      "2        0.010811  0.006353  0.008876  0.002942\n",
      "After Iteration [5000]: weights diff: \n",
      "                0         1         2         3\n",
      "node_id                                        \n",
      "0        0.000823  0.000327  0.000623  0.000235\n",
      "1        0.040897  0.019032  0.032677  0.011951\n",
      "2        0.041720  0.019358  0.033300  0.012185\n",
      "After Iteration [6000]: weights diff: \n",
      "                0         1         2         3\n",
      "node_id                                        \n",
      "0        0.000397  0.000091  0.000402  0.000182\n",
      "1        0.064779  0.029376  0.052071  0.016816\n",
      "2        0.065176  0.029466  0.052473  0.016998\n",
      "After Iteration [7000]: weights diff: \n",
      "                0         1         2         3\n",
      "node_id                                        \n",
      "0        0.000368  0.000287  0.000076  0.000004\n",
      "1        0.017828  0.008228  0.015916  0.004686\n",
      "2        0.018196  0.008515  0.015992  0.004682\n",
      "After Iteration [8000]: weights diff: \n",
      "                0         1         2         3\n",
      "node_id                                        \n",
      "0        0.000706  0.000288  0.000524  0.000200\n",
      "1        0.010941  0.003358  0.007593  0.002119\n",
      "2        0.010235  0.003070  0.007068  0.001919\n",
      "After Iteration [9000]: weights diff: \n",
      "                0         1         2         3\n",
      "node_id                                        \n",
      "0        0.000159  0.000117  0.000015  0.000007\n",
      "1        0.064230  0.027219  0.052670  0.015884\n",
      "2        0.064389  0.027335  0.052684  0.015876\n",
      "After Iteration [10000]: weights diff: \n",
      "                0         1         2         3\n",
      "node_id                                        \n",
      "0        0.000147  0.000145  0.000029  0.000024\n",
      "1        0.051298  0.023093  0.041650  0.011705\n",
      "2        0.051151  0.022948  0.041679  0.011729\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>expected_label</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>setosa</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>versicolor</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>virginica</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   expected_label predicted_label\n",
       "0          setosa          setosa\n",
       "1          setosa          setosa\n",
       "2          setosa          setosa\n",
       "3          setosa          setosa\n",
       "4          setosa          setosa\n",
       "5          setosa          setosa\n",
       "6          setosa          setosa\n",
       "7          setosa          setosa\n",
       "8          setosa          setosa\n",
       "9          setosa          setosa\n",
       "10     versicolor      versicolor\n",
       "11     versicolor      versicolor\n",
       "12     versicolor      versicolor\n",
       "13     versicolor      versicolor\n",
       "14     versicolor      versicolor\n",
       "15     versicolor      versicolor\n",
       "16     versicolor      versicolor\n",
       "17     versicolor      versicolor\n",
       "18     versicolor      versicolor\n",
       "19     versicolor      versicolor\n",
       "20      virginica       virginica\n",
       "21      virginica       virginica\n",
       "22      virginica       virginica\n",
       "23      virginica       virginica\n",
       "24      virginica       virginica\n",
       "25      virginica       virginica\n",
       "26      virginica       virginica\n",
       "27      virginica       virginica\n",
       "28      virginica       virginica\n",
       "29      virginica       virginica"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "class OneLayerNetworkWithSoftMax:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.w, self.bias = None, 0.0\n",
    "        self.optimiser = None\n",
    "        self.output = None\n",
    "        \n",
    "    def init_weights(self, X, Y):\n",
    "        \"\"\"\n",
    "        Initialize a 2D weight matrix as a Dataframe with \n",
    "        dim(n_labels*n_features).         \n",
    "        \"\"\"\n",
    "        self.labels = np.unique(Y)\n",
    "              \n",
    "        w_init = np.random.randn(len(self.labels), X.shape[1])        \n",
    "        self.w = pd.DataFrame(data=w_init)\n",
    "        self.w.index.name = 'node_id'\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Return the predicted label of x using current weights.\n",
    "        \"\"\"\n",
    "        output = self.forward(x, update=False)\n",
    "        max_label_idx = np.argmax(output)\n",
    "        return self.labels[max_label_idx]\n",
    "\n",
    "    def forward(self, x, update=True):\n",
    "        \"\"\"\n",
    "        Calculate softmax(w^Tx+b) for x using current $w_i$ s.\n",
    "        \"\"\"\n",
    "        #output = self.w.apply(lambda row: np.dot(row, x), axis=1)\n",
    "        output = np.dot(self.w, x)\n",
    "        output += self.bias\n",
    "        \n",
    "        output = softmax(output) \n",
    "        if update:\n",
    "            self.output = output        \n",
    "        return output\n",
    "    \n",
    "    def backward(self, x, y, learning_rate):\n",
    "        \"\"\"\n",
    "        Executes the weight update step\n",
    "        \n",
    "            grad = (self.output - y)          \n",
    "            \n",
    "            for i in range(len(grad)):\n",
    "                dw[i] -= grad[i] * x\n",
    "                \n",
    "            w -= learning_rate * dw\n",
    "        \n",
    "        :param x: one sample vector.\n",
    "        :param y: One-hot encoded label for x.\n",
    "        \"\"\"\n",
    "        \n",
    "        # [y_hat1 - y1, y_hat2-y2, ... ]\n",
    "        y_hat_min_y = self.output - y\n",
    "        \n",
    "        # Transpose the above to a column vector\n",
    "        # and then multiply x with each element\n",
    "        # to produce a 2D array (n_labels*n_features), same as w\n",
    "        error_grad = np.apply_along_axis(lambda z: z*x , \n",
    "            1, np.atleast_2d(y_hat_min_y).T)\n",
    "        dw = learning_rate * error_grad\n",
    "        return dw\n",
    "    \n",
    "    def print_weight_diff(self, i, w_old, diff_only=True):\n",
    "        if not diff_only:        \n",
    "            print('Before Iteration [%s]: weights are: \\n%s' % \n",
    "                  (i+1, w_old))\n",
    "\n",
    "            print('After Iteration [%s]: weights are: \\n%s' % \n",
    "                  (i+1, self.w))\n",
    "        \n",
    "        w_diff = np.abs(w_old - self.w)\n",
    "        print('After Iteration [%s]: weights diff: \\n%s' % \n",
    "              (i+1, w_diff))\n",
    "                \n",
    "    def _update_batch(self, i, X_batch, Y_batch, learning_rate, print_every=1000):        \n",
    "        w_old = self.w.copy()\n",
    "        \n",
    "        dw = []            \n",
    "        for x, y in zip(X_batch, Y_batch):\n",
    "            self.forward(x)\n",
    "            dw_item = self.backward(x, y, learning_rate)\n",
    "            dw.append(dw_item)\n",
    "            \n",
    "        dw_batch = np.mean(dw, axis=0)\n",
    "        self.w -= dw_batch\n",
    "\n",
    "        if (i == 0) or ((i+1) % print_every == 0):\n",
    "            self.print_weight_diff(i, w_old)\n",
    "    \n",
    "    def train(self, X, Y, \n",
    "              n_iters=10000, \n",
    "              learning_rate=0.2,\n",
    "              minibatch_size=30,\n",
    "              epsilon=1E-8):\n",
    "        \"\"\"\n",
    "        Entry point for the Minibatch SGD training method.\n",
    "                      \n",
    "        Calls forward+backward for each (x_i, y_i) pair and adjusts the\n",
    "        weight w accordingly.        \n",
    "        \"\"\"\n",
    "        self.init_weights(X, Y)    \n",
    "        Y = encode_1_of_n(self.labels, Y)\n",
    "        \n",
    "        n_samples = X.shape[0]     \n",
    "               \n",
    "        # MiniBatch SGD\n",
    "        for i in range(n_iters):\n",
    "            batch_indices = np.random.randint(0, \n",
    "                n_samples-1, minibatch_size)\n",
    "            \n",
    "            X_batch = X[batch_indices, :]\n",
    "            Y_batch = Y[batch_indices, :]           \n",
    "                    \n",
    "            self._update_batch(i, X_batch, Y_batch, learning_rate)\n",
    "                \n",
    "# Set aside test data\n",
    "label_grouper = iris_df.groupby('species')\n",
    "test = label_grouper.head(10).set_index('species')\n",
    "train = label_grouper.tail(100).set_index('species')\n",
    "\n",
    "# Train the Network\n",
    "X_train, Y_train = train.as_matrix(), train.index.values\n",
    "nn = OneLayerNetworkWithSoftMax()\n",
    "nn.train(X_train, Y_train)\n",
    "\n",
    "# Test\n",
    "results = test.apply(lambda row : nn.predict(row.as_matrix()), axis=1)\n",
    "results.name = 'predicted_label'\n",
    "results.index.name = 'expected_label'\n",
    "\n",
    "results.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient of the Cross Entropy Error\n",
    "\n",
    "**Recap** We know the the cross entropy error is the average of the vector products between the 1-hot enconding of label and the softmax output.\n",
    "\n",
    "$L = - \\frac {1}{n} \\sum_{i=1}^n Y_i^T ln(\\hat Y_i)$\n",
    "\n",
    "Where the sum runs over all of the $n$ input samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a complex derivation, and we need to approach it step-by step. First, let's work out what the $i$-th sample contributes to the gradient of L, i.e. the derivative of - $Y_i^Tln(\\hat Y_i)$.\n",
    "\n",
    "Let's draw the structure of the Network using networkx for a 2-class problem, so we have 2 input nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from matplotlib import pylab\n",
    "\n",
    "G = nx.DiGraph()\n",
    "G.add_edges_from(\n",
    "    [('i', 'n1'), \n",
    "     ('i', 'n2'),\n",
    "     ('n1', 's1'),\n",
    "     ('n2', 's1'),\n",
    "     ('n1', 's2'),\n",
    "     ('n2', 's2'),\n",
    "     ('s1', 'y1'),\n",
    "     ('s2', 'y2'),\n",
    "    ])\n",
    "\n",
    "pos = {'i': (1, 1), \n",
    "       'n1': (2, 0), 'n2': (2, 2),\n",
    "       's1': (3, 0), 's2': (3, 2),\n",
    "       'y1': (4, 0), 'y2': (4, 2),\n",
    "      }\n",
    "\n",
    "labels = {'i': r'$x_i$',\n",
    "         'n1': r'$w_1$', 'n2': r'$w_2$',\n",
    "         's1': r'$s_1$', # r'$\\frac {\\exp(z_{i1})} {S_i}$', \n",
    "         's2': r'$s_2$', # r'$\\frac {\\exp(z_{i2})} {S_i}$'         \n",
    "         }\n",
    "\n",
    "edge_labels = {('i', 'n1'): r'$x_i$', \n",
    "               ('i', 'n2'): r'$x_i$',\n",
    "               ('n1', 's1'): r'$w_1^Tx_i$',\n",
    "               ('n1', 's2'): r'$w_1^Tx_i$', \n",
    "               ('n2', 's1'): r'$w_2^Tx_i$', \n",
    "               ('n2', 's2'): r'$w_2^Tx_i$',\n",
    "               ('n2', 's1'): r'$w_2^Tx_i$',\n",
    "               ('s1', 'y1'): r'$\\frac {\\exp(z_{i1})} {S_i}$',\n",
    "               ('s2', 'y2'): r'$\\frac {\\exp(z_{i2})} {S_i}$',\n",
    "              }\n",
    "nx.draw(G, pos=pos, node_size=1000)\n",
    "nx.draw_networkx_labels(G,pos,labels, font_size=15, color='white')\n",
    "nx.draw_networkx_edge_labels(G, pos=pos, \n",
    "    edge_labels=edge_labels, font_size=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the derivative $- Y_i^Tln(\\hat {Y_i})$, we will break down the vector sum:\n",
    "\n",
    "$L_i = - [y_1 ln (\\hat {y_1}) + y_2 ln (\\hat {y_2}) + ... ]$, where \n",
    "\n",
    "* Each of $(y_1, y_2, ...)$ is an element of the one hot encoded label for sample $x_i$, so only one of them is 1, all the others are 0.\n",
    "* Each of $(\\hat {y_1}, \\hat {y_2}, ...)$ is an element of the softmax output for input $x_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that\n",
    "\n",
    "$\\begin{equation}\n",
    "y_1 ln (\\hat {y_1}) = y_1 ln \\frac {\\exp(z_{i1})} {\\exp(z_{i1}) + \\exp(z_{i2}) + ...} \\\\\n",
    "y_2 ln (\\hat {y_2}) = y_2 ln \\frac {\\exp(z_{i2})} {\\exp(z_{i1}) + \\exp(z_{i2}) + ...} \\\\\n",
    "\\vdots\n",
    "\\end{equation}$\n",
    "\n",
    "Where $z_{i1} = w_1^Tx_i, z_{i2} = w_2^Tx_i$, and so on.\n",
    "\n",
    "Our end goal is to calculate $(\\frac {\\partial L_i}{\\partial w_1}, \\frac {\\partial L_i}{\\partial w_2}, ...)$. We can use the Chain rule to produce:\n",
    "\n",
    "$\\begin{equation}\n",
    "\\frac {\\partial L_i}{\\partial w_1} = \\frac {\\partial L_i} {\\partial z_{i1}} \\frac {\\partial z_{i1}}{\\partial w_1} \\\\\n",
    "\\frac {\\partial L_i}{\\partial w_2} = \\frac {\\partial L_i} {\\partial z_{i2}} \\frac {\\partial z_{i2}}{\\partial w_2} \\\\\n",
    "\\vdots\n",
    "\\end{equation}$\n",
    "\n",
    "The denominator is the same for all of $(\\hat {y_1}, \\hat {y_2}, ...)$, so let's call that $S_i$. \n",
    "\n",
    "$S_i = \\exp(z_{i1}) + \\exp(z_{i1}) + ...$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the equations above become:\n",
    "\n",
    "$\\begin{equation}\n",
    "y_1 ln(\\hat {y_1}) = z_{i1} - ln(S_i) \\\\\n",
    "y_2 ln(\\hat {y_2}) = z_{i2} - ln(S_i) \\\\\n",
    "\\vdots\n",
    "\\end{equation}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the partial derivative of all these equations w.r.t $z_{i1}$, we get:\n",
    "\n",
    "$\\begin{equation}\n",
    "\\frac {y_1} {\\hat {y_1}} \\frac {\\partial \\hat {y_1}} {\\partial z_{i1}} = y_1(1 - \\frac {z_{i1}} {S_i}) = y_1(1 - \\hat {y_1}) \\\\\n",
    "\\frac {y_2} {\\hat {y_2}} \\frac {\\partial \\hat {y_2}} {\\partial z_{i1}} =  - y_2 \\frac {z_{i1}} {S_i} = - y_2 \\hat {y_1} \\\\\n",
    "\\vdots\n",
    "\\end{equation}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we can express $\\frac {\\partial L_i} {\\partial z_{j1}}$ as:\n",
    "\n",
    "$\\frac {\\partial L_i} {\\partial z_{j1}} = [y1(\\hat {y_1} - 1) + y2 \\hat {y_1} + y3 \\hat{y_1}+ ...] = [\\hat {y_1}(y_1 + y_2 + ...) - y_1] = (\\hat {y_1} - y_1)$\n",
    "\n",
    "Since exactly 1 of $(y_1 + y_2 + ...)$ is 1, and all the others are zero.\n",
    "\n",
    "Similarly, we can prove:\n",
    "\n",
    "$\\begin{equation}\n",
    "\\frac {\\partial L_i} {\\partial z_{j2}} = (\\hat {y_2} - y_2) \\\\\n",
    "\\frac {\\partial L_i} {\\partial z_{j3}} = (\\hat {y_3} - y_3) \\\\\n",
    "\\vdots\n",
    "\\end{equation}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noting that \n",
    "\n",
    "$\\begin{equation}\n",
    "\\frac {\\partial z_{i1}} {\\partial w_1} = x_i \\\\\n",
    "\\frac {\\partial z_{i2}} {\\partial w_2} = x_i \\\\\n",
    "\\vdots\n",
    "\\end{equation}$\n",
    "\n",
    "We finally arrive at the result:\n",
    "\n",
    "$\\begin{equation}\n",
    "\\frac {\\partial L_i} {\\partial w_1} = - (\\hat {y_1} - y_1)x_i \\\\\n",
    "\\frac {\\partial L_i} {\\partial w_2} = - (\\hat {y_2} - y_2)x_i \\\\\n",
    "\\vdots\n",
    "\\end{equation}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
